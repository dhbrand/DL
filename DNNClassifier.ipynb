{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing import image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name = 'x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgs_input_fn(filenames, labels=None, perform_shuffle=False, repeat_count=1, batch_size=1):\n",
    "    def _parse_function(filename, label):\n",
    "        image_string = tf.read_file(filename)\n",
    "        image = tf.image.decode_image(image_string, channels=1)\n",
    "        image.set_shape([None, None, None])\n",
    "        image = tf.image.resize_images(image, [70, 60])\n",
    "        image.set_shape([70, 60, 1])\n",
    "        d = dict(zip([input_name], [image])), label\n",
    "        return d\n",
    "    if labels is None:\n",
    "        labels = [0]*len(filenames)\n",
    "    labels = np.array(labels)\n",
    "    # Expand the shape of \"labels\" if necessory\n",
    "    if len(labels.shape) == 1:\n",
    "        labels = np.expand_dims(labels, axis=1)\n",
    "    filenames = tf.constant(filenames)\n",
    "    labels = tf.constant(labels)\n",
    "    labels = tf.cast(labels, tf.float32)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    dataset = dataset.map(_parse_function)\n",
    "    if perform_shuffle:\n",
    "        # Randomizes input using a window of 256 elements (read into memory)\n",
    "        dataset = dataset.shuffle(buffer_size=256)\n",
    "    dataset = dataset.repeat(repeat_count)  # Repeats dataset this # times\n",
    "    dataset = dataset.batch(batch_size)  # Batch size to use\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the directory where the original\n",
    "# dataset was uncompressed\n",
    "original_dataset_dir = './morph/colorSubset'\n",
    "\n",
    "# The directory where we will\n",
    "# store our smaller dataset\n",
    "base_dir = './data/male_vs_female_300'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "train_male_dir = os.path.join(train_dir, 'male')\n",
    "train_female_dir = os.path.join(train_dir, 'female')\n",
    "test_male_dir = os.path.join(test_dir, 'male')\n",
    "test_female_dir = os.path.join(test_dir, 'female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(base_dir, exist_ok=True)\n",
    "# # Directories for our training,\n",
    "# # validation and test splits\n",
    "# os.mkdir(train_dir)\n",
    "# os.mkdir(test_dir)\n",
    "# # Directory with our training male pictures\n",
    "# os.mkdir(train_male_dir)\n",
    "# # Directory with our training female pictures\n",
    "# os.mkdir(train_female_dir)\n",
    "# # Directory with our validation male pictures\n",
    "# os.mkdir(test_male_dir)\n",
    "# # Directory with our validation female pictures\n",
    "# os.mkdir(test_female_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# images = glob.glob('morph/colorSubset/*.JPG')\n",
    "# train = images[0:750]\n",
    "# test = images[751:1000]\n",
    "# # Copy first 750 male images to train_male_dir\n",
    "# fnames = [i for i in train if 'M' in i]\n",
    "# for fname in fnames:\n",
    "#     _, tail = os.path.split(fname)\n",
    "#     src = os.path.join(original_dataset_dir, tail)\n",
    "#     dst = os.path.join(train_male_dir, tail)\n",
    "#     shutil.copyfile(src, dst)\n",
    "# # Copy first 750 male images to train_male_dir\n",
    "# fnames = [i for i in train if 'F' in i]\n",
    "# for fname in fnames:\n",
    "#     _, tail = os.path.split(fname)\n",
    "#     src = os.path.join(original_dataset_dir, tail)\n",
    "#     dst = os.path.join(train_female_dir, tail)\n",
    "#     shutil.copyfile(src, dst)\n",
    "# # Copy next 250 male images to test_male_dir\n",
    "# fnames = [i for i in test if 'M' in i]\n",
    "# for fname in fnames:\n",
    "#     _, tail = os.path.split(fname)\n",
    "#     src = os.path.join(original_dataset_dir, tail)\n",
    "#     dst = os.path.join(test_male_dir, tail)\n",
    "#     shutil.copyfile(src, dst)\n",
    "# # Copy next 250 female images to train_male_dir\n",
    "# fnames = [i for i in test if 'F' in i]\n",
    "# for fname in fnames:\n",
    "#     _, tail = os.path.split(fname)\n",
    "#     src = os.path.join(original_dataset_dir, tail)\n",
    "#     dst = os.path.join(test_female_dir, tail)\n",
    "#     shutil.copyfile(src, dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training male images: 156\ntotal training female images: 44\ntotal test male images: 80\ntotal test female images: 18\n"
     ]
    }
   ],
   "source": [
    "print('total training male images:', len(os.listdir(train_male_dir)))\n",
    "print('total training female images:', len(os.listdir(train_female_dir)))\n",
    "print('total test male images:', len(os.listdir(test_male_dir)))\n",
    "print('total test female images:', len(os.listdir(test_female_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "MALE_LABEL = 0\n",
    "FEMALE_LABEL = 1\n",
    "train_male = [os.path.join(train_male_dir, file_name) for file_name in os.listdir(train_male_dir)]\n",
    "train_female = [os.path.join(train_female_dir, file_name) for file_name in os.listdir(train_female_dir)]\n",
    "train_files = train_male + train_female\n",
    "train_labels = [MALE_LABEL]*len(train_male)+[FEMALE_LABEL]*len(train_female)\n",
    "train_files, train_labels = unison_shuffled_copies(train_files, train_labels)\n",
    "test_male = [os.path.join(test_male_dir, file_name) for file_name in os.listdir(test_male_dir)]\n",
    "test_female = [os.path.join(test_female_dir, file_name) for file_name in os.listdir(test_female_dir)]\n",
    "test_files = test_male + test_female\n",
    "test_labels = [MALE_LABEL]*len(test_male)+[FEMALE_LABEL]*len(test_female)\n",
    "test_files, test_labels = unison_shuffled_copies(test_files, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 70, 60, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADwAAABGCAAAAAC1LqgJAAAKEklEQVR4nI1XS49cx3X+qupU1X30c6Z7ZsghZYrmQ3QYyYqDUECCLLJzYgRxdvkH3nqXRfITvHB2WRvwJhCQhb1I4gB2YMiUHcEyZJmCqQdFcTgzbM6ju++r3ln0zHCGQgCfTXfdul+fr75z6pzT7OeAhEQMAAD4jFsdV98DZwBAeMn86oNAEgDc2UbWFS+/+/8bEeDhwMVq3WV/OBbUAQS5WjBtyWsdo0gAIL7MOIl47hz8/JYxHeA9hP0DPbMzARiXUbIUEER2Kh+YAAMPnmDBCcmf90buHA0C4pd/3yJbbTKALrxAjshnHsGgCAygjoAQALcKYfJgMIAHgdWZPycDgQDoVkKcRMifbsovU3Ble15BBw7A5EBo24tgoi9JTbU6v/SeJAI6BSYQAIBbDsZgFauY4g9n8/7mcMAEg/RIwomVurTiRieaMAGxehJWqXo83H3wSKu82zkIxfBG9BwpQFygwn5C0gMI0IAAAkDMIWUP9yoHnmdKQsD77KZCr7t4CpAORgAQyUm4s6TJ7ncmL6HzTMlWJO/jO6++trhwZADsZwBgMkqnuaJjnmbvS04yb4kPJutzZRcLG6rZlT99vq6rVSBiODuzju5MW6PrnVkuOUnkQhQ6SCrywbJJ/Y++uOfdec8rmpTC2ZNe/GDZy/Miz1meF6WORJHK6aDIr+PtZ+wC2DkHVIH7Feukug9jtEIYawgpo0TFuDfKWNYX5Vem/zYHADgXXnjW6ZQOX/ys04D3UiqpyrKQUlJZFpwDcjpQP73gmYic84xFkbh1nn5RDlvnYgDgnCSekyh9CNxrLd1o+6Mjkl0EGGPWc2tfXN4M5t3YO26trWrnrFRKCQCt994rqTgv9dYvjy05IDrHPI8xBgFwHryJfu+TSRd5YJzbwBvzNNXzubXBey1924TY6Y8/EYkDjMip0wjFCMDH3Z4y3ifwlJD6eQqSQ4GE9N47n/xaWzw7rQe5OQWnAEJIT185Ju/AFF/LMpnDZwISggjee5986sdZOAF3mgAkQwADEAhML52myYDUALT1XHPmUzVd0/WIaD5PUcwfcQBgDPwssThHAoBYU2TpcWT6ymjtQBzY+Ji93yO/vXVtYg5rw8SlrgSQIuyLK8kgoCC04eXRwRLMftJf315rqtlBrJ4MrjShaeXgAJ1qKcgAAPoELJEiCB1cUvnv1wb9eadSt29nMdV1Z0ZpGcIRH/F+6Px0PkgnVfCEto8CFfrNlpL7l5/7Ax5i6DJtrrVN0w3L4den1z/7bybz53bykSDA6IiXulik4kjz/fnN3DXMGmLj8aUn8ya1d2mz+dVnm4uNzXD8RnCnBeUcmCFS+fvbEeXTu18L/9LrH5WDtet9/2hBb38+dV9541BWuTsIB/kZWBht0BH3EEKY9WqwG5P716HtiWk7kHHri9dvvv35n4ds2Pvx9h+N9OTxs42bP3yNeTorBifW+onziX32t+FJ8eoe661zkU93ir/59eM/Kd783TdGJnOb5d+pf+qKWxwAqEXLV10khPGPj66FYrnxq2p7uPEXDxtflMfs8v5r2dOqebw9KKkN9eaj4zvv6jwFOJAEzqrIp+/d2ei6TFfj2Cue68RM0TZ9Xdwc7Y4HTMHy2PZ+8218vM4dAPAQTrHW0ht7I7s9pcnGOHVUk1Z3Dpd7y3lz6YbpOmQCjq99+s7RXW3DS2pLsOmTrohszCTrDWZlqTbqUXyAo7cKu7nQ9aJz0PU//vRwfZUbF8FiyrvhXIw99Vqyt38u7IfDOLp3/73rxRZ1y4XlTeHlfGNVv915sOP1uEf7Kh6UQbDH/p/N5pvHu/8r/vg7a88ngruOLVE9o8v37nwwBQck2I9eoL3n/9PcWEIMGRX8F3bPuWLcrO2//t1P6sL5452GWZ/+crBYtdIXtFMCJ6dn4Kq1VFqx6H9MGPfqN/f/+vZRbo55ddgFk+ZvZTt27aS5nI4YnAhQYX1EQOoOWbt30PrB4PHkQfXuf37sQ6qtc0fG2h/9x2jyHC9y2yM3nsNmgt34bGk7YbQ5Pgom/+oO++aTT39jfnJtuhgcum7hVPyr6y0fA5yf0a41WajATUEf3uIcbvm7pbo9YVfvvjO5lG//12x69yDND7xTWlvZQcKHE8E8Mus5hMkXzFz+97bsmZ1nV662sdJ1dMNL1dYD39w7dMvaDXt/lo0OkYMJeBCPAejAgUCdgn6mqt1X4tFwe3+vGnZXNrrnD9S22JfBN77X1k2JpYRfjatkSJ81fAm4fPNodKBCuN8ffnM6HFe95/7hBzusmi8j8765cAuJrXKb8dXoG5EVjkUpbq1fso8XQmwNrLlW7HatN4TODSxJg3NqA0D08NAAsPFQSpehxrO6rpty7KmHUFgAKRgU3Bl5Eby6YHCAR/bVB2qNjO/gcj1XXrqO8mN+rFwjq1uXIV5MROReHvaq1/ae6HaeAeT0RhYT+W7uRtI7sXt1cGFKXk0G5y1b3OmqnDvnyM8FH483R9qoHjnfhenwApgY4BAEOykJAUD29fenyWM9bTBJxVb79OAVW1eVrUrmL4IBANrZkybAAYRCfVHEOkibSVE/7iKvrK9NWOtHqV4Ge3hwhoAIRGbAUy8c87iUtZRKxABp0VY855Rr5UIREULi50PlIOAAYSVS3nZkvHepkdY1AzAgoJfnwtSjQR0YOk3uhGZM3vsYneOcO7aMTdwk5wX3Lh4uGumcjbwciSKTOs5ubkdPajU9cefcyQh2YnmS5UxkPRk4vEpZAIhIyyEXIvR++/1LZBfnQnUxWNqpwz7lhcR0MrhcKgCSSEurCjcOR/PF3d5pYhCUJTikAOG8NMPjX+443BbcLShkTIyQB8V0xjVXYSPIb91v+Te63dlSkudkmQsr2VL/4fxR6sTh/EoHhHEL9JwjFvKREMvlFl8n6Wf/0BNL6o21W+wtSEMBIhD9uvIH4MZ/vn3z9p5D5omFwDPDCzXYLyXnBRMx8fGyx6AOx2o4OglVYu39KnJmzdHw9TztCEAyRo0CotDZTHgaDzkszMQNTS6YXYxVB+4ptO2iX++3od1/pt660Vbf64YISWjRVzzJUsbGGK2M84EPJnO4tsYbAyXsgpxwOUIzf/W31lza2DJ0Lbynn5zJGYMU1YCo9BycIW0LCyDWXTvujyiE0GK4NP2rs818M3GqFz8Ie1dPwcLMlzRIKfWPwYFZX3QEjLHeOFaRWo7ZWnuUdVcuH48ybphfDyeBDzBESGt8ArAKzo/m98IsGxln+qqZINLGujHtTA5qxMlSsch8cMPdAgAUAliWQ5z+5fn077c+7wPTg5KlfqUEL+dH1OeaDaz0sQmkpctv3wIAMMqSQ9LS2zYa2Oprlw6NUMJSzmLe2pp45IfF+BGKaRjvzRlPPndxjwOA55rFhLozRemlXd789nvmFankcpg0M4Mk/w/l3ltUfVuodgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=60x70 at 0x10EA96CF8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "next_batch = imgs_input_fn(test_files, labels=test_labels, perform_shuffle=True, batch_size=20)\n",
    "with tf.Session() as sess:\n",
    "    first_batch = sess.run(next_batch)\n",
    "x_d = first_batch[0]['x']\n",
    "\n",
    "print(x_d.shape)\n",
    "img = image.array_to_img(x_d[8])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_l = first_batch[1]\n",
    "x_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [tf.feature_column.numeric_column(key='x', shape=4200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'models/DNN4layers', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x181904e240>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                        hidden_units=[100, 50, 25, 10],\n",
    "                                        n_classes=2,\n",
    "                                        model_dir='models/DNN4layers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/DNN4layers/model.ckpt-902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 903 into models/DNN4layers/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 13.173662, step = 903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 81.7152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 12.865875, step = 1003 (1.225 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 1102 into models/DNN4layers/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 12.926989.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x181904e3c8>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.train(\n",
    "   input_fn=lambda: imgs_input_fn(filenames=train_files,\n",
    "                                  labels=train_labels,\n",
    "                                  perform_shuffle=True,\n",
    "                                  repeat_count=20,\n",
    "                                  batch_size=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-03-22-12:44:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/DNN4layers/model.ckpt-1102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2018-03-22-12:44:02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 1102: accuracy = 0.81632656, accuracy_baseline = 0.8163265, auc = 0.5, auc_precision_recall = 0.5918367, average_loss = 0.63242006, global_step = 1102, label/mean = 0.18367347, loss = 0.63242006, prediction/mean = 0.44787952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results\n   accuracy, was: 0.8163265585899353\n   accuracy_baseline, was: 0.8163264989852905\n   auc, was: 0.5\n   auc_precision_recall, was: 0.59183669090271\n   average_loss, was: 0.6324200630187988\n   label/mean, was: 0.18367347121238708\n   loss, was: 0.6324200630187988\n   prediction/mean, was: 0.44787952303886414\n   global_step, was: 1102\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the estimator using our input function.\n",
    "# We should see our accuracy metric below\n",
    "# Tweaking with the params of the model, you can get >99% accuracy\n",
    "evaluation = classifier.evaluate(\n",
    "    input_fn=lambda: imgs_input_fn(test_files, \n",
    "                                   labels=test_labels, \n",
    "                                   perform_shuffle=False,\n",
    "                                   batch_size=1))\n",
    "print(\"Evaluation results\")\n",
    "for key in evaluation:\n",
    "    print(\"   {}, was: {}\".format(key, evaluation[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
