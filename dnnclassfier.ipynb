{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages,\n",
    "realize how we import keras from tensorflow \n",
    "\n",
    "`tensorflow.python.keras`\n",
    "\n",
    "This is new in tensorflow version 1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davehiltbrand/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.keras.preprocessing import image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [tf.feature_column.numeric_column('x',shape=x_d.shape)]\n",
    "#feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[4])]\n",
    "#feature_cols = tf.contrib.learn.infer_real_valued_columns_from_input(x_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_dir:  /Users/davehiltbrand/GitHub/DL/models/morphDNN\nINFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/Users/davehiltbrand/GitHub/DL/models/morphDNN', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1c258e4cc0>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model_dir = os.path.join(os.getcwd(), \"models/morphDNN\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "print(\"model_dir: \",model_dir)\n",
    "classifier = tf.estimator.DNNClassifier(feature_columns=feature_cols,\n",
    "                                        hidden_units=[10, 20, 10],\n",
    "                                        n_classes=2,\n",
    "                                        model_dir=model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input_name is the model's input layer name, we will need it later when building Input function for your estimator. More on that in Input function section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer name\n",
    "input_name = 'DNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the directory where the original\n",
    "# dataset was uncompressed\n",
    "original_dataset_dir = './morph/images'\n",
    "\n",
    "# The directory where we will\n",
    "# store our smaller dataset\n",
    "base_dir = './data/male_vs_female_small'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "train_male_dir = os.path.join(train_dir, 'male')\n",
    "train_female_dir = os.path.join(train_dir, 'female')\n",
    "test_male_dir = os.path.join(test_dir, 'male')\n",
    "test_female_dir = os.path.join(test_dir, 'female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: './data/male_vs_female_small/train'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9c57d8e56726>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Directories for our training,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# validation and test splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Directory with our training male pictures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: './data/male_vs_female_small/train'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "os.makedirs(base_dir, exist_ok=True)\n",
    "# Directories for our training,\n",
    "# validation and test splits\n",
    "os.mkdir(train_dir)\n",
    "os.mkdir(test_dir)\n",
    "# Directory with our training male pictures\n",
    "os.mkdir(train_male_dir)\n",
    "# Directory with our training female pictures\n",
    "os.mkdir(train_female_dir)\n",
    "# Directory with our validation male pictures\n",
    "os.mkdir(test_male_dir)\n",
    "# Directory with our validation female pictures\n",
    "os.mkdir(test_female_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "images = glob.glob('morph/images/*.JPG')\n",
    "train = images[0:750]\n",
    "test = images[751:1000]\n",
    "# Copy first 750 male images to train_male_dir\n",
    "fnames = [i for i in train if 'M' in i]\n",
    "for fname in fnames:\n",
    "    _, tail = os.path.split(fname)\n",
    "    src = os.path.join(original_dataset_dir, tail)\n",
    "    dst = os.path.join(train_male_dir, tail)\n",
    "    shutil.copyfile(src, dst)\n",
    "# Copy first 750 male images to train_male_dir\n",
    "fnames = [i for i in train if 'F' in i]\n",
    "for fname in fnames:\n",
    "    _, tail = os.path.split(fname)\n",
    "    src = os.path.join(original_dataset_dir, tail)\n",
    "    dst = os.path.join(train_female_dir, tail)\n",
    "    shutil.copyfile(src, dst)\n",
    "# Copy next 250 male images to test_male_dir\n",
    "fnames = [i for i in test if 'M' in i]\n",
    "for fname in fnames:\n",
    "    _, tail = os.path.split(fname)\n",
    "    src = os.path.join(original_dataset_dir, tail)\n",
    "    dst = os.path.join(test_male_dir, tail)\n",
    "    shutil.copyfile(src, dst)\n",
    "# Copy next 250 female images to train_male_dir\n",
    "fnames = [i for i in test if 'F' in i]\n",
    "for fname in fnames:\n",
    "    _, tail = os.path.split(fname)\n",
    "    src = os.path.join(original_dataset_dir, tail)\n",
    "    dst = os.path.join(test_female_dir, tail)\n",
    "    shutil.copyfile(src, dst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let's count how many pictures we have in each training split (train/test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training male images: 563\ntotal training female images: 189\ntotal test male images: 187\ntotal test female images: 62\n"
     ]
    }
   ],
   "source": [
    "print('total training male images:', len(os.listdir(train_male_dir)))\n",
    "print('total training female images:', len(os.listdir(train_female_dir)))\n",
    "print('total test male images:', len(os.listdir(test_male_dir)))\n",
    "print('total test female images:', len(os.listdir(test_female_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/male_vs_female_small/train/male\n"
     ]
    }
   ],
   "source": [
    "print(train_male_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to shuffle images along with the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect and shuffle image files with associated labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MALE_LABEL = 0\n",
    "FEMALE_LABEL = 1\n",
    "train_male = [os.path.join(train_male_dir, file_name) for file_name in os.listdir(train_male_dir)]\n",
    "train_female = [os.path.join(train_female_dir, file_name) for file_name in os.listdir(train_female_dir)]\n",
    "train_files = train_male + train_female\n",
    "train_labels = [MALE_LABEL]*len(train_male)+[FEMALE_LABEL]*len(train_female)\n",
    "train_files, train_labels = unison_shuffled_copies(train_files, train_labels)\n",
    "test_male = [os.path.join(test_male_dir, file_name) for file_name in os.listdir(test_male_dir)]\n",
    "test_female = [os.path.join(test_female_dir, file_name) for file_name in os.listdir(test_female_dir)]\n",
    "test_files = test_male + test_female\n",
    "test_labels = [MALE_LABEL]*len(test_male)+[FEMALE_LABEL]*len(test_female)\n",
    "test_files, test_labels = unison_shuffled_copies(test_files, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at first 10 shuffled image files and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/male_vs_female_small/test/female/152117_00F41.JPG'\n './data/male_vs_female_small/test/female/211681_01F32.JPG'\n './data/male_vs_female_small/test/male/066676_3M44.JPG'\n './data/male_vs_female_small/test/male/179647_03M36.JPG'\n './data/male_vs_female_small/test/male/239545_00M24.JPG'\n './data/male_vs_female_small/test/male/120896_1M33.JPG'\n './data/male_vs_female_small/test/male/217685_01M26.JPG'\n './data/male_vs_female_small/test/male/198991_01M31.JPG'\n './data/male_vs_female_small/test/male/159740_03M31.JPG'\n './data/male_vs_female_small/test/male/310291_02M21.JPG']\n[1 0 0 1 0 0 0 0 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(752,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_files[:10])\n",
    "print(train_labels[:10])\n",
    "train_files.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input function\n",
    "When we train our model, we'll need a function that reads the input image files/labels and returns the image data and labels. Estimators require that you create a function of the following format:\n",
    "````\n",
    "def input_fn():\n",
    "    ...<code>...\n",
    "    return ({ 'input_1':[ImagesValues]},\n",
    "            [ImageTypeLogit])\n",
    "```\n",
    "The return value must be a two-element tuple organized as follows: :\n",
    "\n",
    "- The first element must be a dictionary in which each input feature is a key, and then a list of values for the training batch.\n",
    "- The second element is a list of labels for the training batch.\n",
    "### Arguments\n",
    "- **filenames**, an array of image file names\n",
    "- **labels=None**, an array of the image labels for the model. Set to None for inference\n",
    "- **perform_shuffle=False**, useful when training, reads batch_size records, then shuffles (randomizes) their order.\n",
    "- **repeat_count=1**, useful when training, repeat the input data several times for each epoch\n",
    "- **batch_size=1**, reads batch_size records at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgs_input_fn(filenames, labels=None, perform_shuffle=False, repeat_count=1, batch_size=1):\n",
    "    def _parse_function(filename, label):\n",
    "        image_string = tf.read_file(filename)\n",
    "        image = tf.image.decode_image(image_string, channels=1)\n",
    "        image.set_shape([None, None, None])\n",
    "        image = tf.image.resize_images(image, [70, 60])\n",
    "        image.set_shape([70, 60, 1])\n",
    "        #d = dict(zip([input_name], [image])), label\n",
    "        d = image, label\n",
    "        return d\n",
    "    if labels is None:\n",
    "        labels = [0]*len(filenames)\n",
    "    labels = np.array(labels)\n",
    "    # Expand the shape of \"labels\" if necessory\n",
    "    if len(labels.shape) == 1:\n",
    "        labels = np.expand_dims(labels, axis=1)\n",
    "    filenames = tf.constant(filenames)\n",
    "    labels = tf.constant(labels)\n",
    "    labels = tf.cast(labels, tf.float32)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    dataset = dataset.map(_parse_function)\n",
    "    if perform_shuffle:\n",
    "        # Randomizes input using a window of 256 elements (read into memory)\n",
    "        dataset = dataset.shuffle(buffer_size=256)\n",
    "    dataset = dataset.repeat(repeat_count)  # Repeats dataset this # times\n",
    "    dataset = dataset.batch(batch_size)  # Batch size to use\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 70, 60, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=60x70 at 0x1C25C05278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "next_batch = imgs_input_fn(train_files, labels=train_labels, perform_shuffle=True, batch_size=20)\n",
    "with tf.Session() as sess:\n",
    "    first_batch = sess.run(next_batch)\n",
    "x_d = first_batch[0]\n",
    "\n",
    "print(x_d.shape)\n",
    "img = image.array_to_img(x_d[8])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'IteratorGetNext_13:0' shape=(?, 70, 60, 1) dtype=float32>,\n <tf.Tensor 'IteratorGetNext_13:1' shape=(?, 1) dtype=float32>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the first batch's labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n       [1.],\n       [1.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [1.],\n       [0.],\n       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "Estimators require an `input_fn` with no arguments, so we create a function with no arguments using lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "features should be a dictionary of `Tensor`s. Given type: <class 'tensorflow.python.framework.ops.Tensor'>",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-73ad7b694c34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Stop training after \"repeat_count\" iterations of train data (epochs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m classifier.train(\n\u001b[0;32m----> 5\u001b[0;31m    input_fn=lambda: imgs_input_fn(filenames=train_files,\n\u001b[0m\u001b[1;32m      6\u001b[0m                                   \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                   \u001b[0mperform_shuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    741\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[0;32m--> 743\u001b[0;31m           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[1;32m    744\u001b[0m       \u001b[0;31m# Check if the user created a loss summary, and add one if they didn't.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m       \u001b[0;31m# We assume here that the summary is called 'loss'. If it is not, we will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'config'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_fn_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEstimatorSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/canned/dnn.py\u001b[0m in \u001b[0;36m_model_fn\u001b[0;34m(features, labels, mode, config)\u001b[0m\n\u001b[1;32m    322\u001b[0m           \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m           \u001b[0minput_layer_partitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_layer_partitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m           config=config)\n\u001b[0m\u001b[1;32m    325\u001b[0m     super(DNNClassifier, self).__init__(\n\u001b[1;32m    326\u001b[0m         model_fn=_model_fn, model_dir=model_dir, config=config)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/canned/dnn.py\u001b[0m in \u001b[0;36m_dnn_model_fn\u001b[0;34m(features, labels, mode, head, hidden_units, feature_columns, optimizer, activation_fn, dropout, input_layer_partitioner, config)\u001b[0m\n\u001b[1;32m    151\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     raise ValueError('features should be a dictionary of `Tensor`s. '\n\u001b[0;32m--> 153\u001b[0;31m                      'Given type: {}'.format(type(features)))\n\u001b[0m\u001b[1;32m    154\u001b[0m   optimizer = optimizers.get_optimizer_instance(\n\u001b[1;32m    155\u001b[0m       optimizer, learning_rate=_LEARNING_RATE)\n",
      "\u001b[0;31mValueError\u001b[0m: features should be a dictionary of `Tensor`s. Given type: <class 'tensorflow.python.framework.ops.Tensor'>"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Train our model, use the previously function imgs_input_fn\n",
    "# Input to training is a file with training example\n",
    "# Stop training after \"repeat_count\" iterations of train data (epochs)\n",
    "classifier.train(\n",
    "   input_fn=lambda: imgs_input_fn(filenames=train_files,\n",
    "                                  labels=train_labels,\n",
    "                                  perform_shuffle=True,\n",
    "                                  repeat_count=5,\n",
    "                                  batch_size=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "Evaluate our model using the examples contained in test_files and test_labels\n",
    "\n",
    "Return value will contain evaluation_metrics such as: loss & average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-03-15-22:04:24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt-320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2018-03-15-22:05:11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 320: acc = 1.0, global_step = 320, loss = 1.1003451e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results\n   acc, was: 1.0\n   loss, was: 1.1003451305668932e-07\n   global_step, was: 320\n"
     ]
    }
   ],
   "source": [
    "evaluate_results = est_malevsfemale.evaluate(\n",
    "    input_fn=lambda: imgs_input_fn(test_files, \n",
    "                                   labels=test_labels, \n",
    "                                   perform_shuffle=False,\n",
    "                                   batch_size=1))\n",
    "print(\"Evaluation results\")\n",
    "for key in evaluate_results:\n",
    "    print(\"   {}, was: {}\".format(key, evaluate_results[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "To predict we can set the `labels` to None because that is what we will be predicting.\n",
    "\n",
    "Here we only predict the first 10 images in the test_files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_results = est_malevsfemale.predict(\n",
    "    input_fn=lambda: imgs_input_fn(test_files[:10], \n",
    "                                   labels=None, \n",
    "                                   perform_shuffle=False,\n",
    "                                   batch_size=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true,
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt-320\n"
     ]
    }
   ],
   "source": [
    "predict_logits = []\n",
    "for prediction in predict_results:\n",
    "    predict_logits.append(prediction['dense_5'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the prediction result\n",
    "The model correctly classified all 10 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict male: [True, False, False, True, False, False, True, True, False, False]\nActual female: [True, False, False, True, False, False, True, True, False, False]\n"
     ]
    }
   ],
   "source": [
    "predict_is_male = [logit > 0.5 for logit in predict_logits]\n",
    "actual_is_female = [label > 0.5 for label in test_labels[:10]]\n",
    "print(\"Predict male:\",predict_is_male)\n",
    "print(\"Actual female:\",actual_is_female)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.estimator.train_and_evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow release 1.4 also introduces the utility function **tf.estimator.train_and_evaluate**, which simplifies training, evaluation, and exporting Estimator models. This function enables distributed execution for training and evaluation, while still supporting local execution.\n",
    "\n",
    "Notice that the train was build on previous training result when we call the `est_catvsdog.train()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt-320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 321 into /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.2246275e-07, step = 321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 371 into /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 1.04802346e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-03-15-22:30:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt-371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [10/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [20/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [30/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [40/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [50/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [60/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [70/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [80/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [90/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [100/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2018-03-15-22:30:20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 371: acc = 1.0, global_step = 371, loss = 1.0537855e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt-371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 372 into /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.04802346e-07, step = 372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 426 into /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 1.0384188e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-03-15-22:40:26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt-426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [10/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [20/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [30/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [40/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [50/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [60/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [70/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [80/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [90/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [100/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2018-03-15-22:40:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 426: acc = 1.0, global_step = 426, loss = 1.0537855e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt-426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 427 into /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.0384188e-07, step = 427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 481 into /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 1.0384188e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-03-15-22:50:54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt-481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [10/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [20/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [30/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [40/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [50/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [60/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [70/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [80/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [90/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [100/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2018-03-15-22:51:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 481: acc = 1.0, global_step = 481, loss = 1.0537855e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt-481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 482 into /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.0384188e-07, step = 482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 500 into /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 1.0768374e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-03-15-22:54:50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [10/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [20/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [30/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [40/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [50/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [60/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [70/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [80/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [90/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [100/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2018-03-15-22:55:10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 500: acc = 1.0, global_step = 500, loss = 1.0537855e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2118.0085487365723 seconds ---\n"
     ]
    }
   ],
   "source": [
    "train_spec = tf.estimator.TrainSpec(input_fn=lambda: imgs_input_fn(test_files,\n",
    "                                                                   labels=test_labels,\n",
    "                                                                   perform_shuffle=True,\n",
    "                                                                   repeat_count=5,\n",
    "                                                                   batch_size=20), \n",
    "                                    max_steps=500)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=lambda: imgs_input_fn(test_files,\n",
    "                                                                 labels=test_labels,\n",
    "                                                                 perform_shuffle=False,\n",
    "                                                                 batch_size=1))\n",
    "import time\n",
    "start_time = time.time()\n",
    "tf.estimator.train_and_evaluate(est_malevsfemale, train_spec, eval_spec)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features, labels = imgs_input_fn(train_files, labels=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data')\n",
    "\n",
    "def input(dataset):\n",
    "    return dataset.images, dataset.labels.astype(np.int32)\n",
    "\n",
    "# Specify feature\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[28, 28])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'keys'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-230fb4bb01d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Feature columns describe how to use the input.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmy_feature_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmy_feature_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumeric_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'keys'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Feature columns describe how to use the input.\n",
    "my_feature_columns = []\n",
    "for key in next_batch:\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = input(mnist.train)[0]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
