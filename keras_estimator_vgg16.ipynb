{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages,\n",
    "realize how we import keras from tensorflow \n",
    "\n",
    "`tensorflow.python.keras`\n",
    "\n",
    "This is new in tensorflow version 1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davehiltbrand/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras.preprocessing import image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Keras model\n",
    "We are leveraging the pre-trained VGG16 model's convolution layers. aka the \"convolutional base\" of the model. Then we add our own classifier fully connected layers to do binary classification(cat vs dog). \n",
    "\n",
    "Note that since we don't want to touch the parameters pre-trained in the \"convolutional base\", so we set them as not trainable. Want to go deeper how this model works? Check out this great [jupyter notebook](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.3-using-a-pretrained-convnet.ipynb) by the creator of Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(150,150,3))\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nvgg16 (Model)                (None, 4, 4, 512)         14714688  \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 8192)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 256)               2097408   \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 257       \n=================================================================\nTotal params: 16,812,353\nTrainable params: 2,097,665\nNon-trainable params: 14,714,688\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/davehiltbrand/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py:1557: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model to TF estimator\n",
    "`model_dir` will be our location to store trained tensorflow models. Training progress can be viewed by TensorBoard.\n",
    "\n",
    "I found that I have to specify the full path, otherwise, Tensorflow will complain about it later during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_dir:  /Users/davehiltbrand/GitHub/DL/models/morphVGG\nINFO:tensorflow:Using the Keras model from memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/Users/davehiltbrand/GitHub/DL/models/morphVGG', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x181cf66320>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model_dir = os.path.join(os.getcwd(), \"models/morphVGG\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "print(\"model_dir: \",model_dir)\n",
    "est_malevsfemale = tf.keras.estimator.model_to_estimator(keras_model=model,\n",
    "                                                    model_dir=model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input_name is the model's input layer name, we will need it later when building Input function for your estimator. More on that in Input function section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vgg16_input']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input layer name\n",
    "input_name = model.input_names\n",
    "input_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data\n",
    "\n",
    "The cats vs. dogs dataset was made available by Kaggle.com as part of a computer vision \n",
    "competition in late 2013. You can download the original dataset at \n",
    "https://www.kaggle.com/c/dogs-vs-cats/download/train.zip (you will need to create a Kaggle account if you don't already have one -- don't worry, the \n",
    "process is painless).\n",
    "\n",
    "After downloading and uncompressing it, we will create a new dataset containing three subsets: a training set with 1000 samples of each class, and a test set with 500 samples of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the directory where the original\n",
    "# dataset was uncompressed\n",
    "original_dataset_dir = './morph/images'\n",
    "\n",
    "# The directory where we will\n",
    "# store our smaller dataset\n",
    "base_dir = './data/male_vs_female_small'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "train_male_dir = os.path.join(train_dir, 'male')\n",
    "train_female_dir = os.path.join(train_dir, 'female')\n",
    "test_male_dir = os.path.join(test_dir, 'male')\n",
    "test_female_dir = os.path.join(test_dir, 'female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(base_dir, exist_ok=True)\n",
    "# Directories for our training,\n",
    "# validation and test splits\n",
    "os.mkdir(train_dir)\n",
    "os.mkdir(test_dir)\n",
    "# Directory with our training male pictures\n",
    "os.mkdir(train_male_dir)\n",
    "# Directory with our training female pictures\n",
    "os.mkdir(train_female_dir)\n",
    "# Directory with our validation male pictures\n",
    "os.mkdir(test_male_dir)\n",
    "# Directory with our validation female pictures\n",
    "os.mkdir(test_female_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "images = glob.glob('morph/images/*.JPG')\n",
    "train = images[0:750]\n",
    "test = images[751:1000]\n",
    "# Copy first 750 male images to train_male_dir\n",
    "fnames = [i for i in train if 'M' in i]\n",
    "for fname in fnames:\n",
    "    _, tail = os.path.split(fname)\n",
    "    src = os.path.join(original_dataset_dir, tail)\n",
    "    dst = os.path.join(train_male_dir, tail)\n",
    "    shutil.copyfile(src, dst)\n",
    "# Copy first 750 male images to train_male_dir\n",
    "fnames = [i for i in train if 'F' in i]\n",
    "for fname in fnames:\n",
    "    _, tail = os.path.split(fname)\n",
    "    src = os.path.join(original_dataset_dir, tail)\n",
    "    dst = os.path.join(train_female_dir, tail)\n",
    "    shutil.copyfile(src, dst)\n",
    "# Copy next 250 male images to test_male_dir\n",
    "fnames = [i for i in test if 'M' in i]\n",
    "for fname in fnames:\n",
    "    _, tail = os.path.split(fname)\n",
    "    src = os.path.join(original_dataset_dir, tail)\n",
    "    dst = os.path.join(test_male_dir, tail)\n",
    "    shutil.copyfile(src, dst)\n",
    "# Copy next 250 female images to train_male_dir\n",
    "fnames = [i for i in test if 'F' in i]\n",
    "for fname in fnames:\n",
    "    _, tail = os.path.split(fname)\n",
    "    src = os.path.join(original_dataset_dir, tail)\n",
    "    dst = os.path.join(test_female_dir, tail)\n",
    "    shutil.copyfile(src, dst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let's count how many pictures we have in each training split (train/test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training male images: 563\ntotal training female images: 189\ntotal test male images: 187\ntotal test female images: 62\n"
     ]
    }
   ],
   "source": [
    "print('total training male images:', len(os.listdir(train_male_dir)))\n",
    "print('total training female images:', len(os.listdir(train_female_dir)))\n",
    "print('total test male images:', len(os.listdir(test_male_dir)))\n",
    "print('total test female images:', len(os.listdir(test_female_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/male_vs_female_small/train/male\n"
     ]
    }
   ],
   "source": [
    "print(train_male_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to shuffle images along with the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect and shuffle image files with associated labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "MALE_LABEL = 0\n",
    "FEMALE_LABEL = 1\n",
    "train_male = [os.path.join(train_male_dir, file_name) for file_name in os.listdir(train_male_dir)]\n",
    "train_female = [os.path.join(train_female_dir, file_name) for file_name in os.listdir(train_female_dir)]\n",
    "train_files = train_male + train_female\n",
    "train_labels = [MALE_LABEL]*len(train_male)+[FEMALE_LABEL]*len(train_female)\n",
    "train_files, train_labels = unison_shuffled_copies(train_files, train_labels)\n",
    "test_male = [os.path.join(test_male_dir, file_name) for file_name in os.listdir(test_male_dir)]\n",
    "test_female = [os.path.join(test_female_dir, file_name) for file_name in os.listdir(test_female_dir)]\n",
    "test_files = test_male + test_female\n",
    "test_labels = [MALE_LABEL]*len(test_male)+[FEMALE_LABEL]*len(test_female)\n",
    "test_files, test_labels = unison_shuffled_copies(test_files, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at first 10 shuffled image files and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/male_vs_female_small/test/male/254586_01M40.JPG'\n './data/male_vs_female_small/test/female/108101_0F33.JPG'\n './data/male_vs_female_small/test/female/166928_01F29.JPG'\n './data/male_vs_female_small/test/male/343075_01M32.JPG'\n './data/male_vs_female_small/test/female/247498_03F24.JPG'\n './data/male_vs_female_small/test/male/103743_3M37.JPG'\n './data/male_vs_female_small/test/male/275320_02M25.JPG'\n './data/male_vs_female_small/test/female/335180_02F23.JPG'\n './data/male_vs_female_small/test/male/324260_01M40.JPG'\n './data/male_vs_female_small/test/female/165370_02F32.JPG']\n[0 1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(test_files[:10])\n",
    "print(train_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input function\n",
    "When we train our model, we'll need a function that reads the input image files/labels and returns the image data and labels. Estimators require that you create a function of the following format:\n",
    "````\n",
    "def input_fn():\n",
    "    ...<code>...\n",
    "    return ({ 'input_1':[ImagesValues]},\n",
    "            [ImageTypeLogit])\n",
    "```\n",
    "The return value must be a two-element tuple organized as follows: :\n",
    "\n",
    "- The first element must be a dictionary in which each input feature is a key, and then a list of values for the training batch.\n",
    "- The second element is a list of labels for the training batch.\n",
    "### Arguments\n",
    "- **filenames**, an array of image file names\n",
    "- **labels=None**, an array of the image labels for the model. Set to None for inference\n",
    "- **perform_shuffle=False**, useful when training, reads batch_size records, then shuffles (randomizes) their order.\n",
    "- **repeat_count=1**, useful when training, repeat the input data several times for each epoch\n",
    "- **batch_size=1**, reads batch_size records at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgs_input_fn(filenames, labels=None, perform_shuffle=False, repeat_count=1, batch_size=1):\n",
    "    def _parse_function(filename, label):\n",
    "        image_string = tf.read_file(filename)\n",
    "        image = tf.image.decode_image(image_string, channels=3)\n",
    "        image.set_shape([None, None, None])\n",
    "        image = tf.image.resize_images(image, [150, 150])\n",
    "        image = tf.subtract(image, 116.779) # Zero-center by mean pixel\n",
    "        image.set_shape([150, 150, 3])\n",
    "        image = tf.reverse(image, axis=[2]) # 'RGB'->'BGR'\n",
    "        d = dict(zip([input_name], [image])), label\n",
    "        return d\n",
    "    if labels is None:\n",
    "        labels = [0]*len(filenames)\n",
    "    labels = np.array(labels)\n",
    "    # Expand the shape of \"labels\" if necessory\n",
    "    if len(labels.shape) == 1:\n",
    "        labels = np.expand_dims(labels, axis=1)\n",
    "    filenames = tf.constant(filenames)\n",
    "    labels = tf.constant(labels)\n",
    "    labels = tf.cast(labels, tf.float32)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    dataset = dataset.map(_parse_function)\n",
    "    if perform_shuffle:\n",
    "        # Randomizes input using a window of 256 elements (read into memory)\n",
    "        dataset = dataset.shuffle(buffer_size=256)\n",
    "    dataset = dataset.repeat(repeat_count)  # Repeats dataset this # times\n",
    "    dataset = dataset.batch(batch_size)  # Batch size to use\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the input function output\n",
    "Looks like color channels 'RGB' has changed to 'BGR' and shape resized to (150, 150) correctly for our model. That is the input format the VGG16's \"convolutional base\" is expecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 150, 150, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=150x150 at 0x18334B1128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "next_batch = imgs_input_fn(test_files, labels=test_labels, perform_shuffle=True, batch_size=20)\n",
    "with tf.Session() as sess:\n",
    "    first_batch = sess.run(next_batch)\n",
    "x_d = first_batch[0]['vgg16_input']\n",
    "\n",
    "print(x_d.shape)\n",
    "img = image.array_to_img(x_d[8])\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the first batch's labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [1.],\n       [0.],\n       [0.],\n       [1.],\n       [0.],\n       [0.],\n       [1.],\n       [1.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "Estimators require an `input_fn` with no arguments, so we create a function with no arguments using lambda. Suggested you should only attempt it if you have access to a GPU, it only takes couple minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/davehiltbrand/GitHub/DL/models/morphVGG/model.ckpt-3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 4 into /Users/davehiltbrand/GitHub/DL/models/morphVGG/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.5658845, step = 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-e5755efd306b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Stop training after \"repeat_count\" iterations of train data (epochs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m est_malevsfemale.train(\n\u001b[0;32m----> 5\u001b[0;31m    input_fn=lambda: imgs_input_fn(test_files,\n\u001b[0m\u001b[1;32m      6\u001b[0m                                   \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                   \u001b[0mperform_shuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m           \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    816\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    537\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1011\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1014\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1087\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1159\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Train our model, use the previously function imgs_input_fn\n",
    "# Input to training is a file with training example\n",
    "# Stop training after \"repeat_count\" iterations of train data (epochs)\n",
    "est_malevsfemale.train(\n",
    "   input_fn=lambda: imgs_input_fn(test_files,\n",
    "                                  labels=test_labels,\n",
    "                                  perform_shuffle=True,\n",
    "                                  repeat_count=5,\n",
    "                                  batch_size=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "Evaluate our model using the examples contained in test_files and test_labels\n",
    "\n",
    "Return value will contain evaluation_metrics such as: loss & average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-03-15-22:04:24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt-320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2018-03-15-22:05:11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 320: acc = 1.0, global_step = 320, loss = 1.1003451e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results\n   acc, was: 1.0\n   loss, was: 1.1003451305668932e-07\n   global_step, was: 320\n"
     ]
    }
   ],
   "source": [
    "evaluate_results = est_malevsfemale.evaluate(\n",
    "    input_fn=lambda: imgs_input_fn(test_files, \n",
    "                                   labels=test_labels, \n",
    "                                   perform_shuffle=False,\n",
    "                                   batch_size=1))\n",
    "print(\"Evaluation results\")\n",
    "for key in evaluate_results:\n",
    "    print(\"   {}, was: {}\".format(key, evaluate_results[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "To predict we can set the `labels` to None because that is what we will be predicting.\n",
    "\n",
    "Here we only predict the first 10 images in the test_files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_results = est_malevsfemale.predict(\n",
    "    input_fn=lambda: imgs_input_fn(test_files[:10], \n",
    "                                   labels=None, \n",
    "                                   perform_shuffle=False,\n",
    "                                   batch_size=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true,
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt-320\n"
     ]
    }
   ],
   "source": [
    "predict_logits = []\n",
    "for prediction in predict_results:\n",
    "    predict_logits.append(prediction['dense_5'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the prediction result\n",
    "The model correctly classified all 10 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict male: [True, False, False, True, False, False, True, True, False, False]\nActual female: [True, False, False, True, False, False, True, True, False, False]\n"
     ]
    }
   ],
   "source": [
    "predict_is_male = [logit > 0.5 for logit in predict_logits]\n",
    "actual_is_female = [label > 0.5 for label in test_labels[:10]]\n",
    "print(\"Predict male:\",predict_is_male)\n",
    "print(\"Actual female:\",actual_is_female)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.estimator.train_and_evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow release 1.4 also introduces the utility function **tf.estimator.train_and_evaluate**, which simplifies training, evaluation, and exporting Estimator models. This function enables distributed execution for training and evaluation, while still supporting local execution.\n",
    "\n",
    "Notice that the train was build on previous training result when we call the `est_catvsdog.train()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt-320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 321 into /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.2246275e-07, step = 321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 371 into /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 1.04802346e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-03-15-22:30:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt-371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [10/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [20/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [30/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [40/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [50/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [60/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [70/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [80/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [90/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [100/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2018-03-15-22:30:20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 371: acc = 1.0, global_step = 371, loss = 1.0537855e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt-371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 372 into /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.04802346e-07, step = 372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 426 into /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 1.0384188e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-03-15-22:40:26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt-426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [10/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [20/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [30/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [40/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [50/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [60/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [70/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [80/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [90/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [100/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2018-03-15-22:40:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 426: acc = 1.0, global_step = 426, loss = 1.0537855e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt-426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 427 into /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.0384188e-07, step = 427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 481 into /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 1.0384188e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-03-15-22:50:54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt-481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [10/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [20/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [30/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [40/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [50/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [60/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [70/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [80/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [90/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [100/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2018-03-15-22:51:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 481: acc = 1.0, global_step = 481, loss = 1.0537855e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt-481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 482 into /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 1.0384188e-07, step = 482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 500 into /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 1.0768374e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-03-15-22:54:50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/davehiltbrand/GitHub/DL/models/morph/model.ckpt-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [10/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [20/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [30/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [40/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [50/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [60/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [70/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [80/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [90/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [100/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2018-03-15-22:55:10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 500: acc = 1.0, global_step = 500, loss = 1.0537855e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2118.0085487365723 seconds ---\n"
     ]
    }
   ],
   "source": [
    "train_spec = tf.estimator.TrainSpec(input_fn=lambda: imgs_input_fn(test_files,\n",
    "                                                                   labels=test_labels,\n",
    "                                                                   perform_shuffle=True,\n",
    "                                                                   repeat_count=5,\n",
    "                                                                   batch_size=20), \n",
    "                                    max_steps=500)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=lambda: imgs_input_fn(test_files,\n",
    "                                                                 labels=test_labels,\n",
    "                                                                 perform_shuffle=False,\n",
    "                                                                 batch_size=1))\n",
    "import time\n",
    "start_time = time.time()\n",
    "tf.estimator.train_and_evaluate(est_malevsfemale, train_spec, eval_spec)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_string = tf.read_file('colorSubset/046359_1M45.JPG')\n",
    "image = tf.image.decode_image(image_string, channels=3)\n",
    "image.set_shape([None, None, None])\n",
    "image = tf.image.resize_images(image, [150, 150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Squeeze:0' shape=(150, 150, ?) dtype=float32>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
